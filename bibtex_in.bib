@ARTICLE{Devlin2018-um,
  title         = "{BERT}: Pre-training of Deep Bidirectional Transformers for
                   Language Understanding",
  author        = "Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and
                   Toutanova, Kristina",
  month         =  oct,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1810.04805"
}

@ARTICLE{Vaswani2017-ez,
  title         = "Attention Is All You Need",
  author        = "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and
                   Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and
                   Kaiser, Lukasz and Polosukhin, Illia",
  month         =  jun,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1706.03762"
}

@ARTICLE{Ji2021-ie,
  title    = "{DNABERT}: pre-trained Bidirectional Encoder Representations from
              Transformers model for {DNA-language} in genome",
  author   = "Ji, Yanrong and Zhou, Zhihan and Liu, Han and Davuluri, Ramana V",
  journal  = "Bioinformatics",
  pages    = "2020.09.17.301879",
  month    =  feb,
  year     =  2021,
  language = "en"
}

@ARTICLE{Vig_undated-by,
  title  = "{BERTology} Meets Biology: Interpreting Attention in Protein
            Language Models",
  author = "Vig, Jesse and Madani, Ali and Varshney, Lav R and Xiong, Caiming
            and Socher, Richard and Rajani, Nazneen Fatema"
}
